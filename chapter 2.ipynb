{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A : Basic Commands for accessing corpus¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown as cb\n",
    "from nltk.corpus import gutenberg as cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedTaggedCorpusReader in '.../corpora/brown' (not loaded yet)>\n",
      "['_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__load', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_unload', 'subdir']\n"
     ]
    }
   ],
   "source": [
    "print(cb)\n",
    "\n",
    "# directories in nltk\n",
    "print(dir(cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "#Categories\n",
    "print(cb.categories())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n"
     ]
    }
   ],
   "source": [
    "#Name of brown corpus file chunks\n",
    "print(cb.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n"
     ]
    }
   ],
   "source": [
    "#First 20 words of brown corpus¶\n",
    "print(cb.words()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'The', 'jury', 'further', 'said', 'in']\n"
     ]
    }
   ],
   "source": [
    "#20 words of 'News' category stating from 10th word\n",
    "print(cb.words(categories=\"news\")[10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]\n"
     ]
    }
   ],
   "source": [
    "# Extract words from data file with fileid 'cg22'\n",
    "print(cb.words(fileids=\"cg22\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# POS tags for brown corpus\n",
    "print(cb.tagged_words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the brown corpus with fileID: cc12 = 2342\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate the number of words in the brown corpus with fileID: fileidcc12.\n",
    "print(\"Number of words in the brown corpus with fileID: cc12 =\",len(cb.words(fileids=\"cc12\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: The Fulton County Grand Jury said Friday an...>\n"
     ]
    }
   ],
   "source": [
    "# Brown corpus raw text without tags\n",
    "# print(cb.words())\n",
    "raw_text=nltk.Text(cb.words('ca01'))\n",
    "print(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 18 of 18 matches:\n",
      "The Fulton County Grand Jury said Friday an investigation of Atla\n",
      " any irregularities took place . The jury further said in term-end presentment\n",
      "nducted . The September-October term jury had been charged by Fulton Superior \n",
      "f such reports was received '' , the jury said , `` considering the widespread\n",
      "s and the size of this city '' . The jury said it did find that many of Georgi\n",
      "ng and improving them '' . The grand jury commented on a number of other topic\n",
      "s '' . Merger proposed However , the jury said it believes `` these two office\n",
      "The City Purchasing Department , the jury said , `` is lacking in experienced \n",
      "was also recommended by the outgoing jury . It urged that the next Legislature\n",
      "e law may be effected '' . The grand jury took a swipe at the State Welfare De\n",
      " general assistance program '' , the jury said , but the State Welfare Departm\n",
      " burden '' on Fulton taxpayers . The jury also commented on the Fulton ordinar\n",
      "d compensation . Wards protected The jury said it found the court `` has incor\n",
      "om unmeritorious criticisms '' , the jury said . Regarding Atlanta's new multi\n",
      "w multi-million-dollar airport , the jury recommended `` that when the new man\n",
      "minate political influences '' . The jury did not elaborate , but it added tha\n",
      "jail deputies On other matters , the jury recommended that : ( 1 ) Four additi\n",
      "pension plan for city employes . The jury praised the administration and opera\n"
     ]
    }
   ],
   "source": [
    "# Check howmany times a particular word is there in corpus\n",
    "raw_text.concordance(\"jury\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "riday an investigation of Atlanta's recent primary election produced `` no evi\n"
     ]
    }
   ],
   "source": [
    "raw_text.concordance(\"recent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "raw_text.concordance(\"Music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin/np-hl ,/,-hl Texas/np-hl \n",
      "--/-- Committee/nn approval/nn of/in Gov./nn-tl Price/np Daniel's/np$ ``/`` abandoned/vbn property/nn ''/'' act/nn seemed/vbd certain/jj Thursday/nr despite/in the/at adamant/jj protests/nns of/in Texas/np bankers/nns ./.\n",
      "\n",
      "\n",
      "\tDaniel/np personally/rb led/vbd the/at fight/nn for/in the/at measure/nn ,/, which/wdt he/pps had/hvd watered/vbn down/rp considerably/rb since/in its/pp$ rejection/nn by/in two/cd previous/jj Legislatures/nns-tl ,/, in/in a/at public/jj hearing/nn before/in the/at House/nn-tl Committee/nn-tl on/in-tl Revenue/nn-tl and/cc-tl Taxation/nn-tl ./.\n",
      "\n",
      "\n",
      "\tUnder/in committee/nn rules/nns ,/, it/pps went/vbd automatically/rb to/in a/at subcommittee/nn for/in one/cd week/nn ./.\n",
      "But/cc questions/nns with/in which/wdt committee/nn members/nns taunted\n"
     ]
    }
   ],
   "source": [
    "#Raw data from brown corpus\n",
    "raw_content= cb.raw(\"ca02\")\n",
    "# print(raw_content)\n",
    "print(raw_content[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# See files from gutenberg corpus\n",
    "print(cg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Adventures of Buster Bear by Thornton W. Burgess 1920]\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "BUSTER BEAR GOES FISHING\r\n",
      "\r\n",
      "\r\n",
      "Buster Bear yawned as he lay on his comfortable bed of leaves and\r\n",
      "watched the first early morning sunbeams creeping through the Green\r\n",
      "Forest to chase out the Black Shadows. Once more he yawned, and slowly\r\n",
      "got to his feet and shook himself. Then he walked over to a big\r\n",
      "pine-tree, stood up on his hind legs, reached as high up on the trunk of\r\n",
      "the tree as he could, and scratched the bark with his great claws. After\r\n",
      "that he yawned until it seemed as if his jaws would crack, and then sat\r\n",
      "down to think what he wanted for breakfast.\r\n",
      "\r\n",
      "While he sat there, trying to make up his mind what would taste best, he\r\n",
      "was listening to the sounds that told of the waking of all the little\r\n",
      "people who live in the Green Forest. He heard Sammy Jay way off in the\r\n",
      "distance screaming, \"Thief! Thief!\" and grinned. \"I wonder,\" thought\r\n",
      "Buster, \"if some one has stolen Sammy's breakfast, or if he has stolen\r\n",
      "th\n"
     ]
    }
   ],
   "source": [
    "# First 1000 words from gutenberg corpus with fileid burgess-busterbrown.txt\n",
    "print(cg.raw(fileids=\"burgess-busterbrown.txt\")[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84663\n"
     ]
    }
   ],
   "source": [
    "# Calaculate the number of characters in the corpus file id 'burgess-busterbrown.txt'¶\n",
    "print(len(cg.raw(\"burgess-busterbrown.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18963\n"
     ]
    }
   ],
   "source": [
    "#Calaculate the number of words in the corpus file id 'burgess-busterbrown.txt'¶\n",
    "print(len(cg.words(\"burgess-busterbrown.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n"
     ]
    }
   ],
   "source": [
    "# Calaculate the number of sentences in the corpus file id 'burgess-busterbrown.txt'¶\n",
    "print(len(cg.sents(\"burgess-busterbrown.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B : Load your own corpus¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'> <_io.TextIOWrapper name='text_corpus.txt' mode='r' encoding='UTF-8'>\n",
      "<class 'str'>\n",
      "3566\n"
     ]
    }
   ],
   "source": [
    "contents=open(\"text_corpus.txt\")\n",
    "print(type(contents),contents)\n",
    "contents=open(\"text_corpus.txt\").read()\n",
    "print(type(contents))\n",
    "print(len(contents)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Frequency distributions for corpus¶\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260819\n",
      "<FreqDist with 19317 samples and 260819 outcomes>\n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n"
     ]
    }
   ],
   "source": [
    "#Continue\n",
    "# Frequency distributions for corpus¶\n",
    "from nltk.book import *\n",
    "print(len(text1))\n",
    "#frequency of words in text1\n",
    "fdist1=FreqDist(text1)\n",
    "print(fdist1)\n",
    "print(fdist1.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct FrequencyDist:  [(' ', 11), ('e', 6), ('a', 5), ('r', 5), ('h', 4), ('o', 4), ('l', 3), ('s', 3), ('u', 3), ('I', 2), ('b', 2), ('t', 2), ('p', 2), ('m', 1), ('d', 1), ('g', 1), ('v', 1)]\n",
      "\n",
      "with Tokenize:  [('I', 2), ('saurabh', 2), ('hello', 1), ('m', 1), ('the', 1), ('super', 1), ('dooper', 1), ('great', 1), ('love', 1)]\n"
     ]
    }
   ],
   "source": [
    "#doing fdist own\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "sentence=\"hello I m saurabh the super dooper great I love saurabh \"\n",
    "print(\"direct FrequencyDist: \",FreqDist(sentence).most_common(),end=\"\\n\\n\")\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "print(\"with Tokenize: \",FreqDist(tokens).most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 41), ('data', 27), ('of', 25), ('the', 21), ('.', 20), (':', 19), ('you', 17), ('are', 16), ('a', 13), ('attributes', 12), ('and', 12), ('is', 10), ('to', 9), ('or', 8), ('will', 6), ('can', 6), ('kinds', 5), ('that', 5), ('for', 5), ('options', 5), ('numeric', 5), ('as', 4), ('These', 4), ('Examples', 4), ('different', 4), ('two', 4), ('categorical', 4), ('level', 4), ('answer', 4), ('from', 4), ('Question', 4), ('?', 4), ('Good', 4), ('your', 4), ('person', 4), ('formats', 4), ('corpora', 3), ('sub-types', 3), ('on', 3), ('following', 3), ('example', 3), ('given', 3), ('1', 3), ('feel', 3), ('choose', 3), ('All', 3), ('2', 3), ('another', 3), ('In', 3), ('values', 3), ('The', 3), ('either', 3), ('we', 3), ('in', 3), ('weight', 3), ('If', 3), ('file', 3), ('Categorical', 2), ('qualitative', 2), ('more', 2), ('our', 2), ('corpus', 2), ('has', 2), ('such', 2), ('There', 2), ('This', 2), ('type', 2), ('attribute', 2), ('used', 2), ('measure', 2), ('non-numeric', 2), ('concepts', 2), ('satisfaction', 2), ('so', 2), ('How', 2), ('Options', 2), ('Very', 2), ('Bad', 2), ('Now', 2), ('any', 2), ('convert', 2), ('value', 2), ('they', 2), ('hotel', 2), ('service', 2), ('Above', 2), ('average', 2), ('Understanding', 2), ('Corpus', 2), ('Dataset', 2), ('[', 2), (']', 2), ('answers', 2), ('one', 2), ('says', 2), ('be', 2), ('both', 2), ('about', 2), ('So', 2), (\"n't\", 2), ('these', 2), ('What', 2)]\n"
     ]
    }
   ],
   "source": [
    "# appplying on imported text data\n",
    "data_tokenized=nltk.word_tokenize(contents)\n",
    "print(FreqDist(data_tokenized).most_common()[:102])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
